{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Training for Simplified Bike Detection with On-the-Fly Data Generation\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates how to train a U-Net model to segment simplified drawings of bicycles. The key features are:\n",
    "\n",
    "*   **Synthetic Data:** We generate training data programmatically. Each sample consists of:\n",
    "    *   An input image: A simple urban background sketch (street, buildings, lampposts), a distracting sun sketch, and a simplified bike sketch (2 circles, lines for frame).\n",
    "    *   A mask image: A binary mask showing *only* the pixels belonging to the bike.\n",
    "*   **On-the-Fly Generation:** Data is generated in batches as needed during training, avoiding the need to store a large dataset. This also provides virtually infinite unique training examples.\n",
    "*   **Variability:** The bike's position, size, and color (grayscale intensity) vary. The sun's position and brightness also vary. The background details change slightly.\n",
    "*   **PyTorch Implementation:** We use PyTorch to define the U-Net architecture and the training loop.\n",
    "*   **Goal:** Train the U-Net to accurately segment the bike, ignoring the background clutter and the sun.\n",
    "*   **Inference & Visualization:** After training, we demonstrate inference by predicting the mask for a new image and drawing a red bounding box around the detected bike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.notebook import tqdm # Progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Define key parameters for data generation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation Config\n",
    "IMG_SIZE = 128 # prev 128 Keep it smaller for faster training initially (e.g., 128x128)\n",
    "MIN_BIKE_INTENSITY = 150 # Min grayscale value for the bike (0-255)\n",
    "MAX_BIKE_INTENSITY = 255 # Max grayscale value for the bike\n",
    "MIN_SUN_INTENSITY = 180\n",
    "MAX_SUN_INTENSITY = 255\n",
    "ADD_NOISE_PROB = 0.3 # Probability to add salt & pepper noise to input image\n",
    "NOISE_AMOUNT = 0.01\n",
    "\n",
    "# Training Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16 # prev 16 Adjust based on your GPU memory\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20 # prev 20 Start with a moderate number, increase if needed\n",
    "STEPS_PER_EPOCH = 200 # prev 200 Number of batches per epoch (since data is generated on-the-fly)\n",
    "VALIDATION_STEPS = 50 # Number of batches for validation check per epoch\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per Epoch: {STEPS_PER_EPOCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Generation Functions\n",
    "\n",
    "These functions create the synthetic images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(image_array, amount=0.02):\n",
    "    \"\"\"Adds salt and pepper noise to a numpy image array.\"\"\"\n",
    "    noisy_image = np.copy(image_array)\n",
    "    num_pixels = image_array.size\n",
    "    # Salt noise\n",
    "    num_salt = np.ceil(amount * num_pixels * 0.5)\n",
    "    coords = tuple(np.random.randint(0, i - 1, int(num_salt)) for i in image_array.shape)\n",
    "    noisy_image[coords] = 255\n",
    "    # Pepper noise\n",
    "    num_pepper = np.ceil(amount * num_pixels * 0.5)\n",
    "    coords = tuple(np.random.randint(0, i - 1, int(num_pepper)) for i in image_array.shape)\n",
    "    noisy_image[coords] = 0\n",
    "    return noisy_image\n",
    "\n",
    "def draw_urban_background(draw, width, height, street_y):\n",
    "    \"\"\"Draws a simple urban background sketch.\"\"\"\n",
    "    line_color = random.randint(20, 75) # Varying shades of grey for background\n",
    "    line_thickness = 1\n",
    "\n",
    "    # Street level\n",
    "    draw.line([(0, street_y), (width, street_y)], fill=line_color, width=line_thickness + 1)\n",
    "    trottoir_y = street_y + random.randint(3, 6)\n",
    "    if trottoir_y < height:\n",
    "       draw.line([(0, trottoir_y), (width, trottoir_y)], fill=line_color, width=line_thickness)\n",
    "\n",
    "    # Buildings\n",
    "    current_x = 0\n",
    "    min_building_width = max(15, int(width * 0.1))\n",
    "    max_building_width = max(30, int(width * 0.3))\n",
    "\n",
    "    while current_x < width - min_building_width:\n",
    "        line_color = random.randint(50, 120) # Varying shades of grey for background\n",
    "        fill_color = random.randint(45, line_color) # Same for buildings\n",
    "        building_width = random.randint(min_building_width, max_building_width)\n",
    "        building_height = random.randint(int(height*0.2), street_y - 10) # Ensure below top & above street\n",
    "        building_top = street_y - building_height\n",
    "        draw.rectangle(\n",
    "            [(current_x, building_top), (current_x + building_width, street_y)],\n",
    "            outline=line_color, width=line_thickness, fill=fill_color\n",
    "        )\n",
    "        # Simple windows (optional)\n",
    "        if random.random() < 0.7:\n",
    "            win_size = max(2, int(building_width * 0.1))\n",
    "            for wx in range(current_x + 5, current_x + building_width - win_size - 2, win_size*2 + 3):\n",
    "                 for wy in range(building_top + 5, street_y - win_size - 2, win_size*2 + 3):\n",
    "                      if wy + win_size < street_y and wx + win_size < current_x + building_width:\n",
    "                           draw.rectangle([(wx, wy), (wx + win_size, wy + win_size)], outline=line_color, width=1)\n",
    "\n",
    "        current_x += building_width + random.randint(0, 5)\n",
    "\n",
    "    # Lampposts\n",
    "    num_lampposts = random.randint(1, 4)\n",
    "    lamp_height = random.randint(int(height*0.15), int(height*0.3))\n",
    "    lamp_bottom = street_y + 2\n",
    "    for _ in range(num_lampposts):\n",
    "        lamp_x = random.randint(10, width - 10)\n",
    "        lamp_top = max(5, lamp_bottom - lamp_height)\n",
    "        draw.line([(lamp_x, lamp_bottom), (lamp_x, lamp_top)], fill=20, width=line_thickness)\n",
    "        draw.ellipse([(lamp_x - 2, lamp_top - 3), (lamp_x + 2, lamp_top)], fill=line_color)\n",
    "    \n",
    "\n",
    "def draw_sun(draw, width, height, street_y):\n",
    "    \"\"\"Draws a simple sun sketch in the sky.\"\"\"\n",
    "    sun_color = random.randint(MIN_SUN_INTENSITY, MAX_SUN_INTENSITY)\n",
    "    sun_radius = random.randint(max(5, int(width*0.03)), max(15, int(width*0.08)))\n",
    "    # Position in the sky (above buildings/street)\n",
    "    max_y_for_sun = street_y - int(height * 0.1) # Ensure some space above street/buildings\n",
    "    if max_y_for_sun <= sun_radius * 2 : max_y_for_sun = sun_radius*2 + 5 # Avoid edge case\n",
    "\n",
    "    sun_cx = random.randint(sun_radius, width - sun_radius)\n",
    "    sun_cy = random.randint(sun_radius, max(sun_radius + 1, max_y_for_sun - sun_radius)) # Place in upper part\n",
    "\n",
    "    # Draw filled sun circle ONLY on the input image drawing context\n",
    "    draw.ellipse(\n",
    "        (sun_cx - sun_radius, sun_cy - sun_radius, sun_cx + sun_radius, sun_cy + sun_radius),\n",
    "        outline=sun_color\n",
    "    )\n",
    "\n",
    "def draw_bike_simplified(draw_sketch, draw_mask, center_x, center_y, wheel_radius, bike_color, line_thickness=1):\n",
    "    \"\"\"Draws the simplified bike on the sketch (input) and mask (target).\"\"\"\n",
    "    mask_color = 255 # Mask is always white for the bike\n",
    "    mask_line_thickness = max(2, line_thickness * 2, wheel_radius // 4) # Thicker for mask coverage\n",
    "\n",
    "    # Simplified fixed geometry relative to wheel radius\n",
    "    wheel_distance = int(wheel_radius * 3.2)\n",
    "    rear_wheel_cx = center_x - wheel_distance // 2\n",
    "    rear_wheel_cy = center_y\n",
    "    front_wheel_cx = center_x + wheel_distance // 2\n",
    "    front_wheel_cy = center_y\n",
    "\n",
    "    A = (rear_wheel_cx, rear_wheel_cy) # Rear wheel center\n",
    "    E = (front_wheel_cx, front_wheel_cy) # Front wheel center\n",
    "    B = (center_x, center_y + wheel_radius * 0.3) # Bottom bracket approx\n",
    "    C = (rear_wheel_cx + wheel_radius * 0.4, center_y - wheel_radius * 1.8) # Seat approx\n",
    "    D = (front_wheel_cx - wheel_radius * 0.2, center_y - wheel_radius * 1.5) # Handlebar approx\n",
    "\n",
    "    # --- Draw on Sketch (Input Image) ---\n",
    "    # Wheels (thin outline)\n",
    "    draw_sketch.ellipse((A[0]-wheel_radius, A[1]-wheel_radius, A[0]+wheel_radius, A[1]+wheel_radius), outline=bike_color, width=line_thickness)\n",
    "    draw_sketch.ellipse((E[0]-wheel_radius, E[1]-wheel_radius, E[0]+wheel_radius, E[1]+wheel_radius), outline=bike_color, width=line_thickness)\n",
    "    # Frame (thin lines)\n",
    "    draw_sketch.line([A, B], fill=bike_color, width=line_thickness) # Chainstay\n",
    "    draw_sketch.line([B, C], fill=bike_color, width=line_thickness) # Seat tube\n",
    "    draw_sketch.line([C, A], fill=bike_color, width=line_thickness) # Seat stay\n",
    "    draw_sketch.line([C, D], fill=bike_color, width=line_thickness) # Top tube\n",
    "    draw_sketch.line([B, D], fill=bike_color, width=line_thickness) # Down tube\n",
    "    draw_sketch.line([D, E], fill=bike_color, width=line_thickness) # Fork\n",
    "\n",
    "    # --- Draw on Mask (Target Image) ---\n",
    "    # Wheels (filled circles)\n",
    "    draw_mask.ellipse((A[0]-wheel_radius, A[1]-wheel_radius, A[0]+wheel_radius, A[1]+wheel_radius), fill=mask_color)\n",
    "    draw_mask.ellipse((E[0]-wheel_radius, E[1]-wheel_radius, E[0]+wheel_radius, E[1]+wheel_radius), fill=mask_color)\n",
    "    # Frame (thick lines for coverage)\n",
    "    draw_mask.line([A, B], fill=mask_color, width=mask_line_thickness)\n",
    "    draw_mask.line([B, C], fill=mask_color, width=mask_line_thickness)\n",
    "    draw_mask.line([C, A], fill=mask_color, width=mask_line_thickness)\n",
    "    draw_mask.line([C, D], fill=mask_color, width=mask_line_thickness)\n",
    "    draw_mask.line([B, D], fill=mask_color, width=mask_line_thickness)\n",
    "    draw_mask.line([D, E], fill=mask_color, width=mask_line_thickness)\n",
    "\n",
    "\n",
    "def generate_bike_sample(img_size):\n",
    "    \"\"\"Generates one pair of (input_image, mask_image) as NumPy arrays.\"\"\"\n",
    "    width, height = img_size, img_size\n",
    "    input_img = Image.new('L', (width, height), 0) # Black background\n",
    "    mask_img = Image.new('L', (width, height), 0)  # Black background for mask\n",
    "\n",
    "    draw_input = ImageDraw.Draw(input_img)\n",
    "    draw_mask = ImageDraw.Draw(mask_img)\n",
    "\n",
    "    street_y = int(height * random.uniform(0.7, 0.85))\n",
    "\n",
    "    # 2. Draw Sun (only on input image)\n",
    "    draw_sun(draw_input, width, height, street_y)\n",
    "    \n",
    "    # 1. Draw Background elements (street, buildings, etc.)\n",
    "    draw_urban_background(draw_input, width, height, street_y)\n",
    "\n",
    "    \n",
    "\n",
    "    # 3. Draw Bike (on input image and mask)\n",
    "    # Bike parameters\n",
    "    min_radius = max(5, int(width * 0.04))\n",
    "    max_radius = max(10, int(width * 0.1))\n",
    "    wheel_radius = random.randint(min_radius, max_radius)\n",
    "    bike_color = random.randint(MIN_BIKE_INTENSITY, MAX_BIKE_INTENSITY)\n",
    "\n",
    "    # Bike position (mostly horizontal variation, vertically near street)\n",
    "    bike_width_approx = wheel_radius * 3.2 + 2 * wheel_radius\n",
    "    margin_x = int(bike_width_approx / 2) + 5\n",
    "    bike_center_x = random.randint(margin_x, width - margin_x)\n",
    "    # Adjust y so wheels are near the generated street level\n",
    "    bike_center_y = street_y - random.randint(0, wheel_radius // 2) # Slightly above or on the line\n",
    "\n",
    "    draw_bike_simplified(draw_input, draw_mask, bike_center_x, bike_center_y, wheel_radius, bike_color)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    input_array = np.array(input_img)\n",
    "    mask_array = np.array(mask_img)\n",
    "\n",
    "    # 4. Optional Noise (only on input image)\n",
    "    if random.random() < ADD_NOISE_PROB:\n",
    "        input_array = add_noise(input_array, amount=NOISE_AMOUNT)\n",
    "\n",
    "    return input_array, mask_array\n",
    "\n",
    "# Let's test the generator and visualize one sample\n",
    "test_img_np, test_mask_np = generate_bike_sample(IMG_SIZE)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].imshow(test_img_np, cmap='gray')\n",
    "ax[0].set_title('Generated Input Image')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(test_mask_np, cmap='gray')\n",
    "ax[1].set_title('Generated Mask')\n",
    "ax[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input shape: {test_img_np.shape}, Mask shape: {test_mask_np.shape}\")\n",
    "print(f\"Input dtype: {test_img_np.dtype}, Mask dtype: {test_mask_np.dtype}\")\n",
    "print(f\"Input min/max: {np.min(test_img_np)}/{np.max(test_img_np)}\")\n",
    "print(f\"Mask min/max: {np.min(test_mask_np)}/{np.max(test_mask_np)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset and DataLoader\n",
    "\n",
    "We create a custom `Dataset` that uses our `generate_bike_sample` function. The `DataLoader` will then handle batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikeDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for generating bike images and masks on-the-fly.\"\"\"\n",
    "    def __init__(self, img_size, num_samples):\n",
    "        self.img_size = img_size\n",
    "        self.num_samples = num_samples # Effectively, steps per epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Length is the number of samples we want to generate per epoch\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate a new sample each time __getitem__ is called\n",
    "        input_np, mask_np = generate_bike_sample(self.img_size)\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        # Input: Add channel dimension (C, H, W) and normalize to [0, 1]\n",
    "        input_tensor = torch.from_numpy(input_np).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        # Mask: Add channel dimension and normalize to [0, 1] (for BCE loss)\n",
    "        mask_tensor = torch.from_numpy(mask_np).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        return input_tensor, mask_tensor\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "# For validation, we generate a separate set of samples on-the-fly\n",
    "num_workers= 0 # os.cpu_count()//2\n",
    "train_dataset = BikeDataset(IMG_SIZE, STEPS_PER_EPOCH * BATCH_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers) # Shuffle batches each epoch\n",
    "\n",
    "\n",
    "val_dataset = BikeDataset(IMG_SIZE, VALIDATION_STEPS * BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Let's check one batch\n",
    "try:\n",
    "    first_batch_img, first_batch_mask = next(iter(train_loader))\n",
    "    print(\"Successfully loaded one batch.\")\n",
    "    print(f\"Image batch shape: {first_batch_img.shape}\") # Should be [BATCH_SIZE, 1, IMG_SIZE, IMG_SIZE]\n",
    "    print(f\"Mask batch shape: {first_batch_mask.shape}\")\n",
    "    print(f\"Image batch dtype: {first_batch_img.dtype}\") # Should be torch.float32\n",
    "    print(f\"Image batch min/max: {first_batch_img.min():.2f}/{first_batch_img.max():.2f}\") # Should be ~0.0/1.0\n",
    "    print(f\"Mask batch min/max: {first_batch_mask.min():.2f}/{first_batch_mask.max():.2f}\") # Should be 0.0/1.0\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {e}\")\n",
    "    print(\"Check num_workers or data generation logic if issues persist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. U-Net Model Definition\n",
    "\n",
    "Define the U-Net architecture using standard PyTorch modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels) # in_channels already halved by ConvTranspose\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1 is from transpose conv, x2 is skip connection\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        # Pad x1 to match x2's dimensions if needed\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                                    diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1) # Concatenate along channel dimension\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor) # Adjusted for factor\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear) # Adjusted for factor\n",
    "        self.up2 = Up(512, 256 // factor, bilinear) # Adjusted for factor\n",
    "        self.up3 = Up(256, 128 // factor, bilinear) # Adjusted for factor\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits # Return logits for BCEWithLogitsLoss\n",
    "\n",
    "# Instantiate the model\n",
    "# n_channels=1 (grayscale), n_classes=1 (bike or not bike)\n",
    "model = UNet(n_channels=1, n_classes=1).to(DEVICE)\n",
    "\n",
    "# Test with a dummy input\n",
    "dummy_input = torch.randn(BATCH_SIZE, 1, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
    "try:\n",
    "    output = model(dummy_input)\n",
    "    print(f\"Model output shape: {output.shape}\") # Should be [BATCH_SIZE, 1, IMG_SIZE, IMG_SIZE]\n",
    "    print(\"Model definition seems okay.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model forward pass test: {e}\")\n",
    "\n",
    "# Count model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup\n",
    "\n",
    "Define the loss function and optimizer. We use `BCEWithLogitsLoss` which is suitable for binary segmentation and expects raw logits from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# BCEWithLogitsLoss combines Sigmoid layer and BCELoss in one single class.\n",
    "# It's more numerically stable than using a plain Sigmoid followed by BCELoss.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler (optional, but can help)\n",
    "# Reduce LR if validation loss plateaus\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "Train the model using the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs on {DEVICE}...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() # Set model to training mode\n",
    "    epoch_train_loss = 0.0\n",
    "    \n",
    "    # Use tqdm for progress bar on the training loader\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", unit=\"batch\")\n",
    "\n",
    "    for images, masks in train_pbar:\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE) # Target masks should be float [0, 1]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks) # Compare logits with target mask\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader) # len(train_loader) is STEPS_PER_EPOCH\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    epoch_val_loss = 0.0\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\", unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad(): # No need to track gradients during validation\n",
    "        for images, masks in val_pbar:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            epoch_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(val_loader) # len(val_loader) is VALIDATION_STEPS\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Optional: Adjust learning rate with scheduler based on validation loss\n",
    "    # scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Optional: Save model checkpoint periodically or based on best validation loss\n",
    "    # if avg_val_loss < best_val_loss:\n",
    "    #     best_val_loss = avg_val_loss\n",
    "    #     torch.save(model.state_dict(), 'best_unet_model.pth')\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (BCEWithLogits)')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference and Visualization\n",
    "\n",
    "Now, let's use the trained model to predict the mask for a new generated image and draw a bounding box around the detected bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_bbox(mask_np, threshold=0.5):\n",
    "    \"\"\"Calculates the bounding box from a binary mask (NumPy array).\"\"\"\n",
    "    # Ensure mask is binary\n",
    "    binary_mask = (mask_np > threshold).astype(np.uint8)\n",
    "\n",
    "    # Find contours - OpenCV is good for this, but let's use NumPy for simplicity\n",
    "    rows = np.any(binary_mask, axis=1)\n",
    "    cols = np.any(binary_mask, axis=0)\n",
    "    \n",
    "    if not np.any(rows) or not np.any(cols):\n",
    "        return None # No object found\n",
    "\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Return format: (xmin, ymin, xmax, ymax) consistent with PIL draw\n",
    "    return (cmin, rmin, cmax, rmax)\n",
    "\n",
    "def draw_bbox(image_pil, bbox, color=\"red\", thickness=2):\n",
    "    \"\"\"Draws a bounding box on a PIL image.\"\"\"\n",
    "    if bbox is None:\n",
    "        return image_pil # Return original if no bbox\n",
    "        \n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    # The bbox tuple is (xmin, ymin, xmax, ymax)\n",
    "    draw.rectangle(bbox, outline=color, width=thickness)\n",
    "    return image_pil\n",
    "\n",
    "# Generate a few new samples for inference\n",
    "num_inference_samples = 30\n",
    "inference_results = []\n",
    "\n",
    "model.eval() # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_inference_samples):\n",
    "        # 1. Generate new data\n",
    "        input_np, true_mask_np = generate_bike_sample(IMG_SIZE)\n",
    "        \n",
    "        # 2. Prepare for model (convert to tensor, normalize, add batch dim, move to device)\n",
    "        input_tensor = torch.from_numpy(input_np).float().unsqueeze(0).unsqueeze(0) / 255.0\n",
    "        input_tensor = input_tensor.to(DEVICE)\n",
    "\n",
    "        # 3. Get model prediction (logits)\n",
    "        pred_logits = model(input_tensor)\n",
    "\n",
    "        # 4. Convert prediction to probability map (sigmoid) and then to NumPy mask\n",
    "        pred_prob = torch.sigmoid(pred_logits)\n",
    "        pred_mask_np = pred_prob.squeeze().cpu().numpy() # Remove batch and channel dims\n",
    "\n",
    "        # 5. Calculate bounding box from the predicted mask\n",
    "        bbox = mask_to_bbox(pred_mask_np, threshold=0.5) # Use 0.5 threshold\n",
    "\n",
    "        # 6. Prepare images for display (convert input back to PIL)\n",
    "        # Input needs denormalizing and converting back to uint8 if normalized earlier\n",
    "        # Since we only converted to float and divided by 255, multiply back\n",
    "        display_img_np = (input_np).astype(np.uint8)\n",
    "        img_pil = Image.fromarray(display_img_np).convert(\"RGB\") # Convert to RGB for red box\n",
    "\n",
    "        # 7. Draw the bounding box on the PIL image\n",
    "        img_with_bbox = draw_bbox(img_pil, bbox, color=\"red\", thickness=1)\n",
    "\n",
    "        inference_results.append({\n",
    "            \"input_pil\": Image.fromarray(display_img_np), # Original grayscale input\n",
    "            \"true_mask_np\": true_mask_np,\n",
    "            \"pred_mask_np\": pred_mask_np,\n",
    "            \"img_with_bbox\": img_with_bbox,\n",
    "            \"bbox\": bbox\n",
    "        })\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(num_inference_samples, 4, figsize=(16, num_inference_samples * 4))\n",
    "fig.suptitle(\"Inference Results\", fontsize=16)\n",
    "\n",
    "for i, result in enumerate(inference_results):\n",
    "    ax_input = axes[i, 0]\n",
    "    ax_true_mask = axes[i, 1]\n",
    "    ax_pred_mask = axes[i, 2]\n",
    "    ax_bbox = axes[i, 3]\n",
    "\n",
    "    ax_input.imshow(result[\"input_pil\"], cmap='gray')\n",
    "    ax_input.set_title(f\"Input Image {i+1}\")\n",
    "    ax_input.axis('off')\n",
    "\n",
    "    ax_true_mask.imshow(result[\"true_mask_np\"], cmap='gray')\n",
    "    ax_true_mask.set_title(\"True Mask\")\n",
    "    ax_true_mask.axis('off')\n",
    "\n",
    "    im = ax_pred_mask.imshow(result[\"pred_mask_np\"], cmap='viridis', vmin=0, vmax=1) # Show probability map\n",
    "    ax_pred_mask.set_title(\"Predicted Mask (Prob)\")\n",
    "    ax_pred_mask.axis('off')\n",
    "    # fig.colorbar(im, ax=ax_pred_mask, fraction=0.046, pad=0.04) # Optional colorbar\n",
    "\n",
    "    ax_bbox.imshow(result[\"img_with_bbox\"])\n",
    "    ax_bbox.set_title(f\"Prediction w/ BBox\\n{result['bbox']}\") # Display coords\n",
    "    ax_bbox.axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# torch.save(model.state_dict(), 'final_unet_bike_detector.pth')\n",
    "# print(\"Model state dictionary saved to final_unet_bike_detector.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook demonstrated the process of training a U-Net for object segmentation using entirely synthetic, on-the-fly generated data.\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "*   **Synthetic data** can be effective for training, especially for bootstrapping or understanding model behavior on specific features (like shape).\n",
    "*   **On-the-fly generation** is memory-efficient and provides a vast amount of training data, reducing overfitting.\n",
    "*   **Controlling variability** (object color, distractors) is crucial to ensure the model learns the desired features (shape) rather than simple shortcuts (color intensity).\n",
    "*   The U-Net architecture is well-suited for **pixel-wise segmentation tasks**.\n",
    "*   The output mask from the U-Net can be easily post-processed (e.g., thresholding, finding contours) to extract higher-level information like **bounding boxes**.\n",
    "\n",
    "**Potential improvements:**\n",
    "\n",
    "*   More complex backgrounds and bike variations.\n",
    "*   Data augmentation (rotation, scaling, elastic deformations) applied to generated samples.\n",
    "*   Hyperparameter tuning (learning rate, batch size, network depth/width).\n",
    "*   Using more advanced metrics beyond loss (e.g., Dice coefficient, IoU) for evaluation.\n",
    "*   Training for longer or using learning rate scheduling."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.13.7"
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
