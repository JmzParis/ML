{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Training for Simplified Bike Detection with On-the-Fly Data Generation\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook demonstrates how to train a U-Net model to segment simplified drawings of bicycles. The key features are:\n",
    "\n",
    "*   **Synthetic Data:** We generate training data programmatically. Each sample consists of:\n",
    "    *   An input image: A simple urban background sketch (street, buildings, lampposts), a distracting sun sketch, and a simplified bike sketch (2 circles, lines for frame).\n",
    "    *   A mask image: A binary mask showing *only* the pixels belonging to the bike.\n",
    "*   **On-the-Fly Generation:** Data is generated in batches as needed during training, avoiding the need to store a large dataset. This also provides virtually infinite unique training examples.\n",
    "*   **Variability:** The bike's position, size, and color (grayscale intensity) vary. The sun's position and brightness also vary. The background details change slightly.\n",
    "*   **PyTorch Implementation:** We use PyTorch to define the U-Net architecture and the training loop.\n",
    "*   **Goal:** Train the U-Net to accurately segment the bike, ignoring the background clutter and the sun.\n",
    "*   **Inference & Visualization:** After training, we demonstrate inference by predicting the mask for a new image and drawing a red bounding box around the detected bike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm # Progress bar\n",
    "\n",
    "# Import local modules\n",
    "import unet_model as unet\n",
    "import drawing_bike as pic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Define key parameters for data generation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation Config\n",
    "IMG_SIZE = 128 # prev 128 Keep it smaller for faster training initially (e.g., 128x128)\n",
    "ADD_NOISE_PROB = 0.5 # Probability to add salt & pepper noise to input image\n",
    "NOISE_AMOUNT = 0.04 # Amount of noise to add (0.0 to 1.0)\n",
    "\n",
    "# Training Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16 # prev 16 Adjust based on your GPU memory\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20 # prev 20 Start with a moderate number, increase if needed\n",
    "STEPS_PER_EPOCH = 200 # prev 200 Number of batches per epoch (since data is generated on-the-fly)\n",
    "VALIDATION_STEPS = 50 # Number of batches for validation check per epoch\n",
    "\n",
    "MODEL_FILE_PATH = \"../../models/unet/bike_\" # Path to save the trained model\n",
    "FILE_VAL_LOSS = f\"{MODEL_FILE_PATH}val_loss.pth\" # File to save validation loss\n",
    "FILE_MODEL = f\"{MODEL_FILE_PATH}model.pth\" # File to save validation loss\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per Epoch: {STEPS_PER_EPOCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Generation Functions\n",
    "\n",
    "These functions create the synthetic images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic.main(IMG_SIZE) # Initialize the drawing module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Dataset and DataLoader\n",
    "\n",
    "We create a custom `Dataset` that uses our `generate_bike_sample` function. The `DataLoader` will then handle batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikeDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for generating bike images and masks on-the-fly.\"\"\"\n",
    "    def __init__(self, img_size, num_samples):\n",
    "        self.img_size = img_size\n",
    "        self.num_samples = num_samples # Effectively, steps per epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Length is the number of samples we want to generate per epoch\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate a new sample each time __getitem__ is called\n",
    "        input_np, mask_np = pic.generate_bike_sample(self.img_size, self.img_size, ADD_NOISE_PROB, NOISE_AMOUNT)\n",
    "\n",
    "        # Convert NumPy arrays to PyTorch tensors\n",
    "        # Input: Add channel dimension (C, H, W) and normalize to [0, 1]\n",
    "        input_tensor = torch.from_numpy(input_np).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        # Mask: Add channel dimension and normalize to [0, 1] (for BCE loss)\n",
    "        mask_tensor = torch.from_numpy(mask_np).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        return input_tensor, mask_tensor\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "# For validation, we generate a separate set of samples on-the-fly\n",
    "num_workers = 0 if os.name == 'nt' else os.cpu_count()//2  # Zero for Windows compatibility, else use half of available cores\n",
    "train_dataset = BikeDataset(IMG_SIZE, STEPS_PER_EPOCH * BATCH_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers) # Shuffle batches each epoch\n",
    "\n",
    "\n",
    "val_dataset = BikeDataset(IMG_SIZE, VALIDATION_STEPS * BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Let's check one batch\n",
    "try:\n",
    "    first_batch_img, first_batch_mask = next(iter(train_loader))\n",
    "    print(\"Successfully loaded one batch.\")\n",
    "    print(f\"Image batch shape: {first_batch_img.shape}\") # Should be [BATCH_SIZE, 1, IMG_SIZE, IMG_SIZE]\n",
    "    print(f\"Mask batch shape: {first_batch_mask.shape}\")\n",
    "    print(f\"Image batch dtype: {first_batch_img.dtype}\") # Should be torch.float32\n",
    "    print(f\"Image batch min/max: {first_batch_img.min():.2f}/{first_batch_img.max():.2f}\") # Should be ~0.0/1.0\n",
    "    print(f\"Mask batch min/max: {first_batch_mask.min():.2f}/{first_batch_mask.max():.2f}\") # Should be 0.0/1.0\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {e}\")\n",
    "    print(\"Check num_workers or data generation logic if issues persist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. U-Net Model Definition\n",
    "\n",
    "Define the U-Net architecture using standard PyTorch modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# n_channels=1 (grayscale), n_classes=1 (bike or not bike)\n",
    "model = unet.main(device=DEVICE, img_size=IMG_SIZE, n_channels=1, n_classes=1, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup\n",
    "\n",
    "Define the loss function and optimizer. We use `BCEWithLogitsLoss` which is suitable for binary segmentation and expects raw logits from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# BCEWithLogitsLoss combines Sigmoid layer and BCELoss in one single class.\n",
    "# It's more numerically stable than using a plain Sigmoid followed by BCELoss.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler (optional, but can help)\n",
    "# Reduce LR if validation loss plateaus\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "Train the model using the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = torch.load(FILE_VAL_LOSS) if os.path.exists(FILE_VAL_LOSS) else float('inf')\n",
    "if(os.path.exists(FILE_MODEL)):\n",
    "    model.load_state_dict(torch.load(FILE_MODEL))\n",
    "    print(f\"Model loaded successfully. val_loss: {best_val_loss:.5f}\")\n",
    "else:\n",
    "    print(\"No pre-trained model found. Starting from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs on {DEVICE}...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() # Set model to training mode\n",
    "    epoch_train_loss = 0.0\n",
    "    \n",
    "    # Use tqdm for progress bar on the training loader\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", unit=\"batch\")\n",
    "\n",
    "    for images, masks in train_pbar:\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE) # Target masks should be float [0, 1]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks) # Compare logits with target mask\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader) # len(train_loader) is STEPS_PER_EPOCH\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    epoch_val_loss = 0.0\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\", unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad(): # No need to track gradients during validation\n",
    "        for images, masks in val_pbar:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            epoch_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(val_loader) # len(val_loader) is VALIDATION_STEPS\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Optional: Adjust learning rate with scheduler based on validation loss\n",
    "    # scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Optional: Save model checkpoint periodically or based on best validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), FILE_MODEL) # Save model state dict\n",
    "        torch.save(avg_val_loss, FILE_VAL_LOSS)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (BCEWithLogits)')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference and Visualization\n",
    "\n",
    "Now, let's use the trained model to predict the mask for a new generated image and draw a bounding box around the detected bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a few new samples for inference\n",
    "num_inference_samples = 30\n",
    "inference_results = []\n",
    "\n",
    "model.eval() # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_inference_samples):\n",
    "        # 1. Generate new data\n",
    "        input_np, true_mask_np = pic.generate_bike_sample(IMG_SIZE, IMG_SIZE)\n",
    "        \n",
    "        # 2. Prepare for model (convert to tensor, normalize, add batch dim, move to device)\n",
    "        input_tensor = torch.from_numpy(input_np).float().unsqueeze(0).unsqueeze(0) / 255.0\n",
    "        input_tensor = input_tensor.to(DEVICE)\n",
    "\n",
    "        # 3. Get model prediction (logits)\n",
    "        pred_logits = model(input_tensor)\n",
    "\n",
    "        # 4. Convert prediction to probability map (sigmoid) and then to NumPy mask\n",
    "        pred_prob = torch.sigmoid(pred_logits)\n",
    "        pred_mask_np = pred_prob.squeeze().cpu().numpy() # Remove batch and channel dims\n",
    "\n",
    "        # 5. Calculate bounding box from the predicted mask\n",
    "        bbox = pic.mask_to_bbox(pred_mask_np, threshold=0.5) # Use 0.5 threshold\n",
    "\n",
    "        # 6. Prepare images for display (convert input back to PIL)\n",
    "        # Input needs denormalizing and converting back to uint8 if normalized earlier\n",
    "        # Since we only converted to float and divided by 255, multiply back\n",
    "        display_img_np = (input_np).astype(np.uint8)\n",
    "        img_pil = Image.fromarray(display_img_np).convert(\"RGB\") # Convert to RGB for red box\n",
    "\n",
    "        # 7. Draw the bounding box on the PIL image\n",
    "        img_with_bbox = pic.draw_bbox(img_pil, bbox, color=\"red\", thickness=1)\n",
    "\n",
    "        inference_results.append({\n",
    "            \"input_pil\": Image.fromarray(display_img_np), # Original grayscale input\n",
    "            \"true_mask_np\": true_mask_np,\n",
    "            \"pred_mask_np\": pred_mask_np,\n",
    "            \"img_with_bbox\": img_with_bbox,\n",
    "            \"bbox\": bbox\n",
    "        })\n",
    "\n",
    "# Display the results\n",
    "fig, axes = plt.subplots(num_inference_samples, 4, figsize=(16, num_inference_samples * 4))\n",
    "fig.suptitle(\"Inference Results\", fontsize=16)\n",
    "\n",
    "for i, result in enumerate(inference_results):\n",
    "    ax_input = axes[i, 0]\n",
    "    ax_true_mask = axes[i, 1]\n",
    "    ax_pred_mask = axes[i, 2]\n",
    "    ax_bbox = axes[i, 3]\n",
    "\n",
    "    ax_input.imshow(result[\"input_pil\"], cmap='gray')\n",
    "    ax_input.set_title(f\"Input Image {i+1}\")\n",
    "    ax_input.axis('off')\n",
    "\n",
    "    ax_true_mask.imshow(result[\"true_mask_np\"], cmap='gray')\n",
    "    ax_true_mask.set_title(\"True Mask\")\n",
    "    ax_true_mask.axis('off')\n",
    "\n",
    "    im = ax_pred_mask.imshow(result[\"pred_mask_np\"], cmap='viridis', vmin=0, vmax=1) # Show probability map\n",
    "    ax_pred_mask.set_title(\"Predicted Mask (Prob)\")\n",
    "    ax_pred_mask.axis('off')\n",
    "    # fig.colorbar(im, ax=ax_pred_mask, fraction=0.046, pad=0.04) # Optional colorbar\n",
    "\n",
    "    ax_bbox.imshow(result[\"img_with_bbox\"])\n",
    "    ax_bbox.set_title(f\"Prediction w/ BBox\\n{result['bbox']}\") # Display coords\n",
    "    ax_bbox.axis('off')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# torch.save(model.state_dict(), 'final_unet_bike_detector.pth')\n",
    "# print(\"Model state dictionary saved to final_unet_bike_detector.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook demonstrated the process of training a U-Net for object segmentation using entirely synthetic, on-the-fly generated data.\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "*   **Synthetic data** can be effective for training, especially for bootstrapping or understanding model behavior on specific features (like shape).\n",
    "*   **On-the-fly generation** is memory-efficient and provides a vast amount of training data, reducing overfitting.\n",
    "*   **Controlling variability** (object color, distractors) is crucial to ensure the model learns the desired features (shape) rather than simple shortcuts (color intensity).\n",
    "*   The U-Net architecture is well-suited for **pixel-wise segmentation tasks**.\n",
    "*   The output mask from the U-Net can be easily post-processed (e.g., thresholding, finding contours) to extract higher-level information like **bounding boxes**.\n",
    "\n",
    "**Potential improvements:**\n",
    "\n",
    "*   More complex backgrounds and bike variations.\n",
    "*   Data augmentation (rotation, scaling, elastic deformations) applied to generated samples.\n",
    "*   Hyperparameter tuning (learning rate, batch size, network depth/width).\n",
    "*   Using more advanced metrics beyond loss (e.g., Dice coefficient, IoU) for evaluation.\n",
    "*   Training for longer or using learning rate scheduling."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.13.7"
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
