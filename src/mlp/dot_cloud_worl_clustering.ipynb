{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Capacity for Separating Complex 2D Spot Clouds (Swirl/Vortex)\n",
    "\n",
    "This notebook demonstrates how a feed-forward neural network handles a more complex classification task involving 2D spot clouds. We will generate four initial clusters and then apply a non-linear \"swirl\" transformation to distort them, making linear separation impossible.\n",
    "\n",
    "We expect that simple network architectures might struggle, and increasing network capacity (neurons or layers) will be necessary to achieve good separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math # For swirl calculation\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot data and decision boundaries (same as before)\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plots the data points and the decision boundary learned by the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X_np = X.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "\n",
    "    x_min, x_max = X_np[:, 0].min() - 1, X_np[:, 0].max() + 1\n",
    "    y_min, y_max = X_np[:, 1].min() - 1, X_np[:, 1].max() + 1\n",
    "    h = 0.05 # Adjust step size if needed for performance/resolution\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_points_tensor = torch.tensor(grid_points, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Z = model(grid_points_tensor)\n",
    "        # Handle potential multi-class output formats\n",
    "        if Z.shape[1] > 1:\n",
    "             _, predicted = torch.max(Z.data, 1) # For CrossEntropyLoss/multi-class\n",
    "        else:\n",
    "             # This case might not be needed for this specific multi-class notebook, but good practice\n",
    "             predicted = (torch.sigmoid(Z.data) > 0.5).long() # For BCEWithLogitsLoss/binary\n",
    "             predicted = predicted.squeeze() # Remove extra dimension\n",
    "\n",
    "        predicted_np = predicted.cpu().numpy()\n",
    "\n",
    "    Z = predicted_np.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 8)) # Slightly larger figure for complex boundaries\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    scatter = plt.scatter(X_np[:, 0], X_np[:, 1], c=y_np, cmap=plt.cm.Spectral, edgecolors='k', s=20) # Smaller points\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    num_classes = len(np.unique(y_np))\n",
    "    if num_classes <= 10:\n",
    "       handles, labels = scatter.legend_elements()\n",
    "       legend_labels = [f'Cloud {i}' for i in range(num_classes)]\n",
    "       plt.legend(handles, legend_labels, title=\"Classes\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Neural Network Model (same as before)\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper Neural Network Model (same as before)\n",
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
    "        super(DeeperNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function (same as before, returning the trained model)\n",
    "def train_model(model, X_train, y_train, learning_rate=0.01, num_epochs=1000, print_loss_every=100):\n",
    "    \"\"\"Trains the provided model.\"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f\"\\n--- Training Started ---\")\n",
    "    # Get model details dynamically\n",
    "    if isinstance(model, SimpleNet):\n",
    "        print(f\"Model: SimpleNet\")\n",
    "        print(f\"Input Size: {model.fc1.in_features}, Hidden Size: {model.fc1.out_features}, Output Size: {model.fc2.out_features}\")\n",
    "    elif isinstance(model, DeeperNet):\n",
    "        print(f\"Model: DeeperNet\")\n",
    "        print(f\"Input Size: {model.fc1.in_features}, Hidden1: {model.fc1.out_features}, Hidden2: {model.fc2.out_features}, Output Size: {model.fc3.out_features}\")\n",
    "    else:\n",
    "        print(f\"Model: {model.__class__.__name__} (structure not explicitly printed)\")\n",
    "\n",
    "    print(f\"Epochs: {num_epochs}, Learning Rate: {learning_rate}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % print_loss_every == 0 or epoch == 0 or epoch == num_epochs -1:\n",
    "             # Print more frequently or at the end for long training\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(\"--- Training Finished ---\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(X_train)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = y_train.size(0)\n",
    "        correct = (predicted == y_train).sum().item()\n",
    "        print(f'Final Training Accuracy: {(100 * correct / total):.2f} %')\n",
    "    return model # Return trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scenario 4: Four Swirled Clouds\n",
    "\n",
    "We generate four initial blobs and then apply a swirl transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scenario 4: Data Generation ---\n",
    "\n",
    "def swirl_data(X, y, strength=0.75):\n",
    "    \"\"\"Applies a swirl distortion to the data points.\"\"\"\n",
    "    X_swirled = np.zeros_like(X)\n",
    "    center_x, center_y = 0.0, 0.0 # Swirl around the origin\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        x, y_coord = X[i, 0], X[i, 1]\n",
    "        dx = x - center_x\n",
    "        dy = y_coord - center_y\n",
    "        \n",
    "        # Calculate distance and angle from the center\n",
    "        dist = np.sqrt(dx*dx + dy*dy)\n",
    "        angle = np.arctan2(dy, dx)\n",
    "        \n",
    "        # Apply swirl: rotation angle increases with distance\n",
    "        swirl_angle = strength * dist / 5.0 # Adjust divisor to control swirl tightness\n",
    "        \n",
    "        new_angle = angle + swirl_angle\n",
    "        \n",
    "        # Convert back to Cartesian coordinates\n",
    "        X_swirled[i, 0] = center_x + dist * np.cos(new_angle)\n",
    "        X_swirled[i, 1] = center_y + dist * np.sin(new_angle)\n",
    "        \n",
    "    return X_swirled, y # Labels remain the same\n",
    "\n",
    "# Parameters for initial blobs\n",
    "N_SAMPLES = 500 # More samples might help define the boundaries better\n",
    "N_FEATURES = 2\n",
    "N_CLASSES_4 = 4\n",
    "# Start with centers relatively far out, the swirl will pull them in/around\n",
    "CENTERS_4_initial = [(-4, -4), (4, 4), (-4, 4), (4, -4)]\n",
    "CLUSTER_STD_4 = 0.7 # Keep initial clusters relatively tight\n",
    "\n",
    "# Generate initial blobs\n",
    "X_initial, y_initial = make_blobs(n_samples=N_SAMPLES,\n",
    "                                  n_features=N_FEATURES,\n",
    "                                  centers=CENTERS_4_initial,\n",
    "                                  cluster_std=CLUSTER_STD_4,\n",
    "                                  random_state=42)\n",
    "\n",
    "# Apply the swirl transformation\n",
    "SWIRL_STRENGTH = 5 # Controls how much rotation/distortion is applied (try 5)\n",
    "X4, y4 = swirl_data(X_initial, y_initial, strength=SWIRL_STRENGTH)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors and move to device\n",
    "X4_tensor = torch.tensor(X4, dtype=torch.float32).to(device)\n",
    "y4_tensor = torch.tensor(y4, dtype=torch.long).to(device)\n",
    "\n",
    "print(f\"Generated {X4.shape[0]} swirled samples for {N_CLASSES_4} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw swirled data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X4[:, 0], X4[:, 1], c=y4, cmap=plt.cm.Spectral, edgecolors='k', s=15)\n",
    "plt.title(f'Scenario 4: Raw Data ({N_CLASSES_4} Swirled Clouds, Strength={SWIRL_STRENGTH})')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.axhline(0, color='grey', lw=0.5)\n",
    "plt.axvline(0, color='grey', lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the clouds are now distorted and intertwined, making simple linear or polygonal separation impossible.\n",
    "\n",
    "### 2.1 Attempt with a Shallow Network (Increased Neurons)\n",
    "\n",
    "Let's see how the `SimpleNet` (one hidden layer) performs, but we'll give it significantly more neurons than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scenario 4a: Training Shallow but Wider Network ---\n",
    "\n",
    "# Model Parameters\n",
    "INPUT_SIZE = N_FEATURES\n",
    "HIDDEN_SIZE_4a = 32 # Significantly more neurons needed (try 5)\n",
    "OUTPUT_SIZE_4 = N_CLASSES_4\n",
    "\n",
    "# Instantiate the model\n",
    "model4a = SimpleNet(INPUT_SIZE, HIDDEN_SIZE_4a, OUTPUT_SIZE_4).to(device)\n",
    "\n",
    "# Train the model - May need significantly more epochs and potentially smaller LR\n",
    "# Increased epochs because the optimization landscape is more complex\n",
    "model4a = train_model(model4a, X4_tensor, y4_tensor,\n",
    "                     num_epochs=5000, # Increased epochs\n",
    "                     learning_rate=0.005, # Possibly smaller LR\n",
    "                     print_loss_every=500)\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(model4a, X4_tensor, y4_tensor,\n",
    "                       title=f\"Scenario 4a: Decision Boundary (SimpleNet, Hidden={HIDDEN_SIZE_4a})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shallow network, even with increased width, might struggle to capture the complex curves accurately. It might create somewhat reasonable boundaries but likely misclassifies points where the swirls are tightest or overlap. The accuracy will likely be lower than in previous scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Attempt with a Deeper Network\n",
    "\n",
    "A deeper network might be better suited to learning the hierarchical features needed to untangle the swirl. Let's try the `DeeperNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scenario 4b: Training Deeper Network ---\n",
    "\n",
    "# Model Parameters for Deeper Network\n",
    "HIDDEN_SIZE1_4b = 24 # Reasonable number for first layer\n",
    "HIDDEN_SIZE2_4b = 12 # Second hidden layer\n",
    "\n",
    "# Instantiate the model\n",
    "model4b = DeeperNet(INPUT_SIZE, HIDDEN_SIZE1_4b, HIDDEN_SIZE2_4b, OUTPUT_SIZE_4).to(device)\n",
    "\n",
    "# Train the model - Again, likely needs many epochs\n",
    "model4b = train_model(model4b, X4_tensor, y4_tensor,\n",
    "                     num_epochs=6000, # Maybe even more epochs\n",
    "                     learning_rate=0.005,\n",
    "                     print_loss_every=500)\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(model4b, X4_tensor, y4_tensor,\n",
    "                       title=f\"Scenario 4b: Decision Boundary (DeeperNet, Layers: {HIDDEN_SIZE1_4b}-{HIDDEN_SIZE2_4b})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion (Swirled Data)\n",
    "\n",
    "This swirled dataset presents a significantly more challenging task for the neural network.\n",
    "\n",
    "- **Shallow Network (Wider):** While increasing the number of neurons in a single hidden layer (`SimpleNet`) provides more capacity than a very narrow network, it often struggles to capture highly non-linear, curved boundaries like those in the swirl pattern. The boundaries learned might be jagged approximations or fail in complex regions.\n",
    "- **Deeper Network:** The `DeeperNet`, with multiple hidden layers, is generally better equipped to handle such complex geometric transformations. The hierarchical processing allows it to potentially learn intermediate representations that help untangle the swirls before the final classification. We typically see smoother and more accurate decision boundaries compared to the shallow network for this type of data, likely achieving higher training accuracy.\n",
    "\n",
    "This demonstrates that for datasets where classes are separated by complex, non-linear boundaries, increasing network **depth** (adding layers) can be more effective than just increasing **width** (adding neurons to a single layer), although often a combination of both is optimal. The increased complexity also necessitates more training data (or augmentation) and longer training times (more epochs) for the model to converge effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
